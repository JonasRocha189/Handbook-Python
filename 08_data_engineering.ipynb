{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1864d127",
   "metadata": {},
   "source": [
    "# Section 8 — Data Engineering\n",
    "Concepts and practical snippets for databases, ETL/ELT, and handling large datasets.\n",
    "\n",
    "**Topics:** SQL basics, `sqlite3`, SQLAlchemy, (conceptual) BigQuery, ETL/ELT patterns, chunked I/O, and simple parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d119c",
   "metadata": {},
   "source": [
    "## 8.1 Databases & SQL — Concepts\n",
    "- **Relational Databases (RDBMS):** tables, rows, columns, keys (PK/FK).\n",
    "- **SQL:** `SELECT`, `WHERE`, `JOIN`, `GROUP BY`, `ORDER BY`, subqueries, window functions.\n",
    "- **When to use what:**\n",
    "  - `sqlite3` for local prototyping & tests.\n",
    "  - **SQLAlchemy** for ORM/connection management.\n",
    "  - **BigQuery** (or cloud warehouses) for analytics at scale.\n",
    "\n",
    "Below: runnable `sqlite3` examples and SQLAlchemy basics (with SQLite)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1099e208",
   "metadata": {},
   "source": [
    "### SQL Cheat Sheet (Quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d24565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrative (not executed as SQL here):\n",
    "sql_select = '''\n",
    "SELECT city, SUM(sales) AS total_sales\n",
    "FROM transactions\n",
    "WHERE date >= '2025-01-01'\n",
    "GROUP BY city\n",
    "ORDER BY total_sales DESC;\n",
    "'''\n",
    "print(sql_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a00ea",
   "metadata": {},
   "source": [
    "## 8.2 `sqlite3` — Lightweight Local Database\n",
    "Create a database in memory, create a table, insert data, and run queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084bc385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# In-memory database\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create table\n",
    "cur.execute('''CREATE TABLE transactions (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    city TEXT,\n",
    "    sales REAL,\n",
    "    date TEXT\n",
    ")''')\n",
    "conn.commit()\n",
    "\n",
    "# Insert sample rows\n",
    "rows = [\n",
    "    (1, 'Montreal', 120.0, '2025-01-01'),\n",
    "    (2, 'Toronto', 200.0, '2025-01-01'),\n",
    "    (3, 'Montreal', 170.0, '2025-02-01'),\n",
    "    (4, 'Vancouver', 150.0, '2025-01-15'),\n",
    "]\n",
    "cur.executemany('INSERT INTO transactions VALUES (?, ?, ?, ?)', rows)\n",
    "conn.commit()\n",
    "\n",
    "# Query with SQL\n",
    "query = \"SELECT city, SUM(sales) AS total_sales FROM transactions GROUP BY city ORDER BY total_sales DESC;\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3067e44",
   "metadata": {},
   "source": [
    "## 8.3 SQLAlchemy — Engine & ORM Basics (with SQLite)\n",
    "Use SQLAlchemy to create an engine, execute SQL, or define models (ORM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b4a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "engine = create_engine('sqlite:///:memory:', echo=False)\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text('CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, city TEXT)'))\n",
    "    conn.execute(text(\"INSERT INTO users (id, name, city) VALUES (1, 'Alice', 'Montreal'), (2, 'Bob', 'Toronto')\"))\n",
    "    result = conn.execute(text('SELECT * FROM users'))\n",
    "    print(result.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57edd0f",
   "metadata": {},
   "source": [
    "> **Tip:** With SQLAlchemy ORM you can define Python classes mapped to tables and let the ORM handle CRUD operations. For analytics, many teams use Core/SQL expressions or raw SQL for clarity and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0fd2ca",
   "metadata": {},
   "source": [
    "## 8.4 BigQuery (Conceptual)\n",
    "- **BigQuery** is a serverless, highly scalable cloud data warehouse by Google.\n",
    "- You can query massive datasets with standard SQL and pay per data scanned.\n",
    "- Python access typically uses the **`google-cloud-bigquery`** client library and service account credentials.\n",
    "- Typical flow:\n",
    "  1) Create a GCP project & dataset; 2) authenticate (env var `GOOGLE_APPLICATION_CREDENTIALS`);\n",
    "  3) Use the client to run queries / load data from GCS; 4) write results to tables or export to storage.\n",
    "\n",
    "_Not executed here due to credentials, but snippet shown below for reference._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e8b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference snippet (not executed here):\n",
    "bigquery_example = r'''\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "query_job = client.query(\"\"\"\n",
    "    SELECT city, SUM(sales) AS total_sales\n",
    "    FROM `project.dataset.transactions`\n",
    "    WHERE date >= '2025-01-01'\n",
    "    GROUP BY city\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\")\n",
    "for row in query_job:\n",
    "    print(row)\n",
    "'''\n",
    "print(bigquery_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38f3f9b",
   "metadata": {},
   "source": [
    "## 8.5 ETL vs ELT — Patterns & Best Practices\n",
    "- **ETL** (Extract → Transform → Load): transform data before loading into the warehouse.\n",
    "- **ELT** (Extract → Load → Transform): load raw data first, then transform in the warehouse (common in modern stacks).\n",
    "\n",
    "**Best Practices:**\n",
    "- Use idempotent steps (safe re-runs).\n",
    "- Keep raw data immutable.\n",
    "- Parameterize configs (env vars / YAML).\n",
    "- Log & monitor jobs.\n",
    "- Version your transformations (SQL or code)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9993f01",
   "metadata": {},
   "source": [
    "### Simple ETL Skeleton (in Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9837c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "RAW = Path('data/raw'); RAW.mkdir(parents=True, exist_ok=True)\n",
    "PROC = Path('data/processed'); PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def extract() -> pd.DataFrame:\n",
    "    # toy extract: create a DataFrame\n",
    "    return pd.DataFrame({\n",
    "        'city': ['Montreal','Toronto','Montreal'],\n",
    "        'sales': [120, 200, 170],\n",
    "        'date': ['2025-01-01','2025-01-01','2025-02-01']\n",
    "    })\n",
    "\n",
    "def transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n",
    "\n",
    "def load(df: pd.DataFrame, path: Path):\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "df_raw = extract()\n",
    "df_t = transform(df_raw)\n",
    "load(df_t, PROC / 'transactions.csv')\n",
    "df_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee6610d",
   "metadata": {},
   "source": [
    "## 8.6 Chunked I/O — Handling Larger Files in Pandas\n",
    "Read and process large CSVs in chunks to reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13468f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a sample CSV with many rows (synthetic, small here)\n",
    "path = Path('data/processed/large_sample.csv')\n",
    "pd.DataFrame({'x': range(1000), 'y': range(1000, 2000)}).to_csv(path, index=False)\n",
    "\n",
    "total = 0\n",
    "for chunk in pd.read_csv(path, chunksize=200):\n",
    "    total += chunk['y'].sum()\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc8db34",
   "metadata": {},
   "source": [
    "## 8.7 Simple Parallelization Example (CPU-bound)\n",
    "Use `multiprocessing` to parallelize CPU-bound tasks (e.g., transformations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "import math\n",
    "\n",
    "def heavy_fn(n: int) -> float:\n",
    "    # Example: compute something moderately heavy\n",
    "    return sum(math.sqrt(i) for i in range(n))\n",
    "\n",
    "with Pool(processes=min(4, cpu_count())) as pool:\n",
    "    results = pool.map(heavy_fn, [10_000, 20_000, 30_000, 40_000])\n",
    "results[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28961f",
   "metadata": {},
   "source": [
    "> **Notes**\n",
    "- For large-scale data, consider distributed tools (e.g., **Dask**, **Spark**).\n",
    "- Profile I/O vs CPU bottlenecks; optimize accordingly.\n",
    "- Use columnar formats (Parquet) for analytics workloads."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
